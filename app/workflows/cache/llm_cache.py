# app/workflows/cache/llm_cache.py

async def get_cached(prompt: str):
    return None   # No cache

async def set_cached(prompt: str, data: dict):
    return None   # No cache
